{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    df = pd.read_csv(\"spotify_million_song_dataset.csv\", nrows=10000)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back In A us Made Car  \n",
      "(Back In The ussr-Beatles)  \n",
      "  \n",
      "Ohhh...  \n",
      "Used to drive a Honda C-I-V-I-C  \n",
      "Didn't sleep a wink at night  \n",
      "Auto-workers laid-off in Detroit last week  \n",
      "Man it's such a dreadful sight  \n",
      "  \n",
      "I'm back in a us made car  \n",
      "You know how lazy we are, boys  \n",
      "Back in a us made car  \n",
      "  \n",
      "Power steering, power windows, seats and brakes  \n",
      "It's bigger than the Astrodome  \n",
      "Drive it under 50 or the back-end shakes  \n",
      "God I hope it gets me home  \n",
      "  \n",
      "I'm back in a us made car  \n",
      "You know how lazy we are, boys  \n",
      "Back in a us  \n",
      "Back in the Do-Less  \n",
      "Back in a us made car  \n",
      "  \n",
      "Those foreign cars really knock me out  \n",
      "They leave Chevettes behind  \n",
      "They cruise right past all the screams and shouts  \n",
      "From gm unemployment li-li  \n",
      "Li-li-li-li-li-li-lines  \n",
      "  \n",
      "Oh tow that Iacocca monster to my house  \n",
      "You didn't have to twist my arm  \n",
      "It's my patriotic duty to be helping out  \n",
      "Can't let the big three buy the farm  \n",
      "  \n",
      "I'm back in us made car  \n",
      "A gallon don't go too far boy  \n",
      "Back in a us made car\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_csv()\n",
    "print(df[\"text\"][1658])\n",
    "len(df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Cleaning and Tokenizing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
    "              'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself',\n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself',\n",
    "              'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    "              'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    "              'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
    "              'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
    "              'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once',\n",
    "              'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
    "              'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "              's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    "              'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\",\n",
    "              'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    "              \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "              'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def load_data(df):\n",
    "    tokenized_songs = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for index, song in df.iterrows():\n",
    "        song_lyrics_paragraphs = song[\"text\"].replace(\"\\n\\n\", \"\").split(\"\\n  \\n\")\n",
    "\n",
    "        tokenized_song = []\n",
    "\n",
    "        for paragraph in song_lyrics_paragraphs:\n",
    "            cleaned_paragraph = paragraph.replace(\"\\n\", \"\").replace(\"  \", \" \")\n",
    "            tokenized_paragraph = tokenize(cleaned_paragraph)\n",
    "            tokenized_filtered_paragraph = []\n",
    "\n",
    "            for token in tokenized_paragraph:\n",
    "                if token not in stop_words:\n",
    "                    tokenized_filtered_paragraph.append(token)\n",
    "                    total_tokens += 1\n",
    "\n",
    "            tokenized_song.append(tokenized_filtered_paragraph)\n",
    "\n",
    "        tokenized_songs.append(tokenized_song)\n",
    "\n",
    "    print(\"Total number of words without stop words:\", total_tokens)\n",
    "    return tokenized_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words without stop words: 1106211\n",
      "Total number of songs: 10000\n"
     ]
    }
   ],
   "source": [
    "tokenized_songs = load_data(df)\n",
    "\n",
    "print(\"Total number of songs:\", len(tokenized_songs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Mapping words to ids and ids to words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokenized_songs):\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    all_tokens_cache = set()\n",
    "\n",
    "    for tokenized_song in tokenized_songs:\n",
    "        for tokenized_para in tokenized_song:\n",
    "            for token in tokenized_para:\n",
    "                all_tokens_cache.add(token)\n",
    "\n",
    "    for index, word in enumerate(all_tokens_cache):\n",
    "        word_to_id[word] = index\n",
    "        id_to_word[index] = word\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique words:  36605\n",
      "Mapping preview:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'indulgence',\n",
       " 1: 'means',\n",
       " 2: 'recorded',\n",
       " 3: 'frat',\n",
       " 4: 'rent',\n",
       " 5: 'drugstore',\n",
       " 6: 'jerk',\n",
       " 7: 'skin',\n",
       " 8: 'busier',\n",
       " 9: 'motored',\n",
       " 10: 'officially',\n",
       " 11: 'wong',\n",
       " 12: 'readeth',\n",
       " 13: \"muthafuckas'll\",\n",
       " 14: 'incidentally',\n",
       " 15: 'singt',\n",
       " 16: 'duo',\n",
       " 17: 'nissi',\n",
       " 18: 'wanders',\n",
       " 19: 'augmented',\n",
       " 20: 'intricately',\n",
       " 21: 'wingtip',\n",
       " 22: \"way'\",\n",
       " 23: 'bouche',\n",
       " 24: 'dionne',\n",
       " 25: 'upheaval',\n",
       " 26: 'imma',\n",
       " 27: 'praises',\n",
       " 28: 'forcast',\n",
       " 29: \"sighin'\",\n",
       " 30: \"moun'\",\n",
       " 31: 'performer',\n",
       " 32: 'theprojects',\n",
       " 33: 'cassidy',\n",
       " 34: 'ben',\n",
       " 35: 'saws',\n",
       " 36: 'poodle',\n",
       " 37: 'buffer',\n",
       " 38: 'gears',\n",
       " 39: 'biggity',\n",
       " 40: \"dependin'\",\n",
       " 41: 'tupelo',\n",
       " 42: 'change',\n",
       " 43: 'tattooed',\n",
       " 44: 'rockstar',\n",
       " 45: \"'where\",\n",
       " 46: \"other's\",\n",
       " 47: 'florida',\n",
       " 48: \"ain't\",\n",
       " 49: 'perico',\n",
       " 50: 'yeh',\n",
       " 51: 'colorful',\n",
       " 52: 'trails',\n",
       " 53: 'gouge',\n",
       " 54: 'mot',\n",
       " 55: 'exciety',\n",
       " 56: 'bogie',\n",
       " 57: 'ceremonies',\n",
       " 58: 'fridges',\n",
       " 59: 'unmerited',\n",
       " 60: \"approachin'\",\n",
       " 61: 'optimistically',\n",
       " 62: 'impulsion',\n",
       " 63: 'endearing',\n",
       " 64: 'giardino',\n",
       " 65: 'spot',\n",
       " 66: \"executin'\",\n",
       " 67: 'hoere',\n",
       " 68: 'lacrosse',\n",
       " 69: 'warehouse',\n",
       " 70: 'talikalang',\n",
       " 71: 'cod',\n",
       " 72: 'gingham',\n",
       " 73: 'meet',\n",
       " 74: 'transmitting',\n",
       " 75: 'omnipotens',\n",
       " 76: 'honey',\n",
       " 77: 'receiving',\n",
       " 78: 'glaring',\n",
       " 79: \"party's\",\n",
       " 80: 'brandishing',\n",
       " 81: 'fingers',\n",
       " 82: 'kaya',\n",
       " 83: 'squared',\n",
       " 84: 'provocative',\n",
       " 85: 'forwardto',\n",
       " 86: 'marie',\n",
       " 87: '12th',\n",
       " 88: 'dovey',\n",
       " 89: 'musk',\n",
       " 90: 'returning',\n",
       " 91: 'booked',\n",
       " 92: 'okies',\n",
       " 93: 'makikita',\n",
       " 94: 'discard',\n",
       " 95: 'certificate',\n",
       " 96: 'pagsisikap',\n",
       " 97: \"streamin'\",\n",
       " 98: 'festering',\n",
       " 99: 'headed',\n",
       " 100: 'craft',\n",
       " 101: 'formy',\n",
       " 102: 'nuits',\n",
       " 103: 'neukkim',\n",
       " 104: 'dimaggio',\n",
       " 105: \"p's\",\n",
       " 106: 'harpo',\n",
       " 107: 'hillbilly',\n",
       " 108: 'garfunkel',\n",
       " 109: 'disguised',\n",
       " 110: 'forbidden',\n",
       " 111: 'jemand',\n",
       " 112: \"jessie's\",\n",
       " 113: 'dove',\n",
       " 114: \"homie's\",\n",
       " 115: 'pressing',\n",
       " 116: 'powdered',\n",
       " 117: 'hammerlock',\n",
       " 118: 'milking',\n",
       " 119: 'tehse',\n",
       " 120: 'caper',\n",
       " 121: 'encounter',\n",
       " 122: 'affray',\n",
       " 123: 'rocka',\n",
       " 124: 'wavelength',\n",
       " 125: \"revolution's\",\n",
       " 126: 'pause',\n",
       " 127: 'raheem',\n",
       " 128: 'yesl',\n",
       " 129: 'lice',\n",
       " 130: 'unconsciousness',\n",
       " 131: 'relative',\n",
       " 132: 'neoya',\n",
       " 133: 'mink',\n",
       " 134: 'policy',\n",
       " 135: \"'pag\",\n",
       " 136: 'looseleaf',\n",
       " 137: 'blon',\n",
       " 138: \"hi'way\",\n",
       " 139: 'worries',\n",
       " 140: 'riven',\n",
       " 141: 'intermittently',\n",
       " 142: 'cancion',\n",
       " 143: \"neighbor's\",\n",
       " 144: 'chambermaid',\n",
       " 145: 'uppa',\n",
       " 146: 'tinseltown',\n",
       " 147: 'sera',\n",
       " 148: 'vulture',\n",
       " 149: 'ode',\n",
       " 150: 'paano',\n",
       " 151: 'heba',\n",
       " 152: 'bases',\n",
       " 153: 'thins',\n",
       " 154: 'swine',\n",
       " 155: 'wily',\n",
       " 156: 'jimmy',\n",
       " 157: 'motherfucker',\n",
       " 158: 'grizz',\n",
       " 159: 'stare',\n",
       " 160: 'safest',\n",
       " 161: \"bouncin'\",\n",
       " 162: 'iaalay',\n",
       " 163: 'pandangan',\n",
       " 164: 'argyles',\n",
       " 165: \"pavlo's\",\n",
       " 166: 'reconstructed',\n",
       " 167: 'mason',\n",
       " 168: 'bualadh',\n",
       " 169: 'pagkakataon',\n",
       " 170: 'fermera',\n",
       " 171: 'ol',\n",
       " 172: 'sooth',\n",
       " 173: 'suspicious',\n",
       " 174: 'gigolo',\n",
       " 175: \"n'finisse\",\n",
       " 176: \"grandpa's\",\n",
       " 177: 'opponents',\n",
       " 178: 'wondering',\n",
       " 179: 'pas',\n",
       " 180: 'brim',\n",
       " 181: 'sender',\n",
       " 182: 'mameland',\n",
       " 183: 'uglyest',\n",
       " 184: 'wynette',\n",
       " 185: 'vapors',\n",
       " 186: 'anchor',\n",
       " 187: 'odometers',\n",
       " 188: 'thous',\n",
       " 189: 'fluids',\n",
       " 190: 'occupied',\n",
       " 191: \"jackin'\",\n",
       " 192: 'hanes',\n",
       " 193: 'ganun',\n",
       " 194: 'teach',\n",
       " 195: 'lug',\n",
       " 196: 'jam',\n",
       " 197: 'jitneys',\n",
       " 198: 'queens',\n",
       " 199: 'speaker',\n",
       " 200: \"living's\",\n",
       " 201: 'brewery',\n",
       " 202: 'spacey',\n",
       " 203: 'thunderbirds',\n",
       " 204: 'california',\n",
       " 205: 'zero',\n",
       " 206: 'ite',\n",
       " 207: 'poupon',\n",
       " 208: 'frauds',\n",
       " 209: \"jill's\",\n",
       " 210: 'fulfilled',\n",
       " 211: 'lovers',\n",
       " 212: 'glass',\n",
       " 213: 'plows',\n",
       " 214: 'global',\n",
       " 215: 'complexity',\n",
       " 216: 'ming',\n",
       " 217: 'rabbitts',\n",
       " 218: \"testin'\",\n",
       " 219: 'kookie',\n",
       " 220: \"dealing'\",\n",
       " 221: 'aids',\n",
       " 222: 'putih',\n",
       " 223: 'dam',\n",
       " 224: \"'coz\",\n",
       " 225: 'upright',\n",
       " 226: 'burning',\n",
       " 227: 'sustained',\n",
       " 228: 'breaks',\n",
       " 229: 'alcohol',\n",
       " 230: 'kitted',\n",
       " 231: 'untroubled',\n",
       " 232: \"we've\",\n",
       " 233: 'choked',\n",
       " 234: 'cockleshells',\n",
       " 235: \"arm's\",\n",
       " 236: 'extinguishers',\n",
       " 237: 'cle',\n",
       " 238: 'androids',\n",
       " 239: 'thekid',\n",
       " 240: 'flits',\n",
       " 241: 'spokane',\n",
       " 242: 'bonnets',\n",
       " 243: 'givens',\n",
       " 244: 'lifestyles',\n",
       " 245: 'denn',\n",
       " 246: 'concluded',\n",
       " 247: 'spread',\n",
       " 248: 'divide',\n",
       " 249: 'doc',\n",
       " 250: 'pluck',\n",
       " 251: 'nicholas',\n",
       " 252: 'chug',\n",
       " 253: 'bunmyeong',\n",
       " 254: 'apply',\n",
       " 255: 'meronta',\n",
       " 256: 'chokers',\n",
       " 257: 'perfumed',\n",
       " 258: 'cycle',\n",
       " 259: 'communist',\n",
       " 260: 'fiya',\n",
       " 261: 'alleluia',\n",
       " 262: 'hustle',\n",
       " 263: 'aida',\n",
       " 264: 'grid',\n",
       " 265: 'pistol',\n",
       " 266: 'porcupine',\n",
       " 267: 'undead',\n",
       " 268: 'blanket',\n",
       " 269: 'g4',\n",
       " 270: 'mistresses',\n",
       " 271: 'edges',\n",
       " 272: 'everglow',\n",
       " 273: 'flag',\n",
       " 274: \"cook's\",\n",
       " 275: \"jammin'\",\n",
       " 276: 'ransom',\n",
       " 277: \"explainin'\",\n",
       " 278: 'fue',\n",
       " 279: 'franticly',\n",
       " 280: 'revolting',\n",
       " 281: 'demo',\n",
       " 282: 'sauna',\n",
       " 283: 'wednesday',\n",
       " 284: 'withthe',\n",
       " 285: 'bionic',\n",
       " 286: 'airplanes',\n",
       " 287: 'jackets',\n",
       " 288: 'elle',\n",
       " 289: 'scabs',\n",
       " 290: 'temporary',\n",
       " 291: 'huggin',\n",
       " 292: 'uh',\n",
       " 293: 'provin',\n",
       " 294: 'throughinterscope',\n",
       " 295: 'recognized',\n",
       " 296: 'fatigues',\n",
       " 297: 'eversince',\n",
       " 298: 'outstretched',\n",
       " 299: 'hommes',\n",
       " 300: 'mattered',\n",
       " 301: 'misunderstand',\n",
       " 302: 'standing',\n",
       " 303: 'floosy',\n",
       " 304: 'samurai',\n",
       " 305: 'squalls',\n",
       " 306: 'blend',\n",
       " 307: 'covering',\n",
       " 308: 'pin',\n",
       " 309: 'mamy',\n",
       " 310: 'dumb',\n",
       " 311: \"severin'\",\n",
       " 312: 'sustain',\n",
       " 313: 'performs',\n",
       " 314: 'continents',\n",
       " 315: 'public',\n",
       " 316: 'spooky',\n",
       " 317: \"strivin'\",\n",
       " 318: \"hot'n\",\n",
       " 319: 'cosmopolitan',\n",
       " 320: 'reappear',\n",
       " 321: 'fingernails',\n",
       " 322: \"s's\",\n",
       " 323: 'eliza',\n",
       " 324: 'las',\n",
       " 325: 'ijust',\n",
       " 326: 'noone',\n",
       " 327: 'tryine',\n",
       " 328: 'tercinta',\n",
       " 329: 'jolene',\n",
       " 330: 'connoisseur',\n",
       " 331: 'aunt',\n",
       " 332: 'demonstrate',\n",
       " 333: 'corset',\n",
       " 334: 'hecate',\n",
       " 335: 'kindness',\n",
       " 336: 'nakal',\n",
       " 337: \"there've\",\n",
       " 338: 'carnie',\n",
       " 339: 'redeem',\n",
       " 340: 'malimot',\n",
       " 341: \"'i'd\",\n",
       " 342: 'steve',\n",
       " 343: 'bunnies',\n",
       " 344: 'stalking',\n",
       " 345: 'society',\n",
       " 346: 'flipflops',\n",
       " 347: 'mauved',\n",
       " 348: 'velma',\n",
       " 349: 'incapable',\n",
       " 350: 'nighttime',\n",
       " 351: 'perverts',\n",
       " 352: 'bargained',\n",
       " 353: 'anggun',\n",
       " 354: 'eyebrow',\n",
       " 355: 'loathing',\n",
       " 356: 'earlye',\n",
       " 357: 'footsore',\n",
       " 358: 'brittany',\n",
       " 359: 'names',\n",
       " 360: 'kurikaeshite',\n",
       " 361: \"again'\",\n",
       " 362: \"drankin'\",\n",
       " 363: 'connecting',\n",
       " 364: 'solider',\n",
       " 365: 'harrowing',\n",
       " 366: \"don'tchange\",\n",
       " 367: 'dealing',\n",
       " 368: 'comeon',\n",
       " 369: 'haste',\n",
       " 370: 'fools',\n",
       " 371: 'kakapit',\n",
       " 372: 'turnip',\n",
       " 373: 'miser',\n",
       " 374: 'untile',\n",
       " 375: 'liveliness',\n",
       " 376: 'breezeway',\n",
       " 377: 'fatherly',\n",
       " 378: \"'tane\",\n",
       " 379: 'remnants',\n",
       " 380: 'dessus',\n",
       " 381: 'gangster',\n",
       " 382: 'posey',\n",
       " 383: 'thewheels',\n",
       " 384: 'pipe',\n",
       " 385: 'hurries',\n",
       " 386: 'beta',\n",
       " 387: 'menamong',\n",
       " 388: 'crazier',\n",
       " 389: 'groan',\n",
       " 390: 'penalty',\n",
       " 391: 'compro',\n",
       " 392: 'mmmmmm',\n",
       " 393: \"exploitin'\",\n",
       " 394: 'ssy',\n",
       " 395: 'believer',\n",
       " 396: 'stoodup',\n",
       " 397: 'fireman',\n",
       " 398: 'hubcaps',\n",
       " 399: 'emerged',\n",
       " 400: 'clinking',\n",
       " 401: 'cuirasses',\n",
       " 402: \"comin'\",\n",
       " 403: 'diabolic',\n",
       " 404: 'loveyou',\n",
       " 405: 'offering',\n",
       " 406: 'aftertaste',\n",
       " 407: 'overestimated',\n",
       " 408: 'precipitation',\n",
       " 409: 'seedy',\n",
       " 410: 'retell',\n",
       " 411: 'tang',\n",
       " 412: 'unencumbered',\n",
       " 413: 'toasting',\n",
       " 414: 'destinies',\n",
       " 415: 'carelessly',\n",
       " 416: 'peggy',\n",
       " 417: 'thisguy',\n",
       " 418: 'oooooh',\n",
       " 419: 'hop',\n",
       " 420: 'ralla',\n",
       " 421: 'slurs',\n",
       " 422: 'declaration',\n",
       " 423: 'barsha',\n",
       " 424: 'nagkaroon',\n",
       " 425: 'ciao',\n",
       " 426: 'reserve',\n",
       " 427: \"try'na\",\n",
       " 428: 'cauliflowers',\n",
       " 429: 'signifies',\n",
       " 430: 'everlasting',\n",
       " 431: 'jars',\n",
       " 432: 'girlish',\n",
       " 433: 'vitality',\n",
       " 434: 'alfie',\n",
       " 435: 'newton',\n",
       " 436: 'car',\n",
       " 437: \"sister's\",\n",
       " 438: 'superstition',\n",
       " 439: 'concept',\n",
       " 440: 'compromising',\n",
       " 441: 'chumps',\n",
       " 442: 'guilded',\n",
       " 443: 'retail',\n",
       " 444: 'gangmul',\n",
       " 445: 'rabbitt',\n",
       " 446: 'quietude',\n",
       " 447: 'itanong',\n",
       " 448: 'win',\n",
       " 449: 'avant',\n",
       " 450: 'hollers',\n",
       " 451: 'yelled',\n",
       " 452: 'theyre',\n",
       " 453: \"'forget\",\n",
       " 454: 'revlon',\n",
       " 455: 'fail',\n",
       " 456: 'jigeumi',\n",
       " 457: 'despised',\n",
       " 458: 'undarum',\n",
       " 459: 'dabeun',\n",
       " 460: 'eunuch',\n",
       " 461: 'gallery',\n",
       " 462: 'mentality',\n",
       " 463: 'violently',\n",
       " 464: 'appart',\n",
       " 465: 'dicks',\n",
       " 466: 'gunne',\n",
       " 467: 'eoptda',\n",
       " 468: 'baiter',\n",
       " 469: 'jiggle',\n",
       " 470: 'minivans',\n",
       " 471: 'contraction',\n",
       " 472: 'ha',\n",
       " 473: \"mir's\",\n",
       " 474: 'actresses',\n",
       " 475: 'mio',\n",
       " 476: 'butts',\n",
       " 477: 'disproportionate',\n",
       " 478: 'holdingmine',\n",
       " 479: 'lad',\n",
       " 480: 'waltzers',\n",
       " 481: 'spaz',\n",
       " 482: \"n'penasais\",\n",
       " 483: 'advertised',\n",
       " 484: 'grovel',\n",
       " 485: 'selman',\n",
       " 486: 'sang',\n",
       " 487: 'broom',\n",
       " 488: 'panoramic',\n",
       " 489: 'birth',\n",
       " 490: 'grave',\n",
       " 491: 'tenir',\n",
       " 492: 'uns',\n",
       " 493: 'tracheotomy',\n",
       " 494: 'fits',\n",
       " 495: 'tailors',\n",
       " 496: 'seductive',\n",
       " 497: 'nandito',\n",
       " 498: 'takea',\n",
       " 499: 'hankerchiefs',\n",
       " 500: 'braincells',\n",
       " 501: 'resist',\n",
       " 502: 'coercion',\n",
       " 503: 'glisting',\n",
       " 504: 'longing',\n",
       " 505: 'turtle',\n",
       " 506: 'advising',\n",
       " 507: 'ardor',\n",
       " 508: 'slender',\n",
       " 509: 'sniffy',\n",
       " 510: 'hanna',\n",
       " 511: 'condescending',\n",
       " 512: 'majestic',\n",
       " 513: 'spattered',\n",
       " 514: 'gied',\n",
       " 515: 'jacques',\n",
       " 516: \"growlin'\",\n",
       " 517: 'kasama',\n",
       " 518: 'attachments',\n",
       " 519: \"high'n'low\",\n",
       " 520: 'etchings',\n",
       " 521: 'rollin',\n",
       " 522: 'intermission',\n",
       " 523: 'choices',\n",
       " 524: 'caddy',\n",
       " 525: 'reconsider',\n",
       " 526: 'kitsch',\n",
       " 527: 'unitl',\n",
       " 528: 'louboutins',\n",
       " 529: 'spinningwheel',\n",
       " 530: \"infant's\",\n",
       " 531: 'apartment',\n",
       " 532: 'clandestine',\n",
       " 533: 'competitions',\n",
       " 534: 'produce',\n",
       " 535: 'receding',\n",
       " 536: \"janitor's\",\n",
       " 537: 'weenie',\n",
       " 538: 'bushland',\n",
       " 539: 'got',\n",
       " 540: 'singles',\n",
       " 541: 'whitman',\n",
       " 542: 'dominus',\n",
       " 543: 'barley',\n",
       " 544: 'secure',\n",
       " 545: 'atman',\n",
       " 546: 'youaround',\n",
       " 547: 'kick',\n",
       " 548: 'lick',\n",
       " 549: 'vanities',\n",
       " 550: 'howshould',\n",
       " 551: 'centre',\n",
       " 552: 'shiney',\n",
       " 553: 'photographed',\n",
       " 554: 'lembut',\n",
       " 555: 'setengah',\n",
       " 556: 'blurted',\n",
       " 557: 'decker',\n",
       " 558: \"'peaceful'\",\n",
       " 559: 'tacks',\n",
       " 560: 'equality',\n",
       " 561: 'plowing',\n",
       " 562: 'clis',\n",
       " 563: 'g450',\n",
       " 564: \"cold'll\",\n",
       " 565: 'buses',\n",
       " 566: 'enfants',\n",
       " 567: 'scremaing',\n",
       " 568: \"prayin'\",\n",
       " 569: 'homewards',\n",
       " 570: 'interests',\n",
       " 571: 'clutches',\n",
       " 572: 'struts',\n",
       " 573: \"i'vebeen\",\n",
       " 574: 'recommended',\n",
       " 575: 'calculated',\n",
       " 576: 'mockery',\n",
       " 577: 'doo',\n",
       " 578: 'barbwire',\n",
       " 579: 'nagbabago',\n",
       " 580: 'bids',\n",
       " 581: 'seville',\n",
       " 582: 'second',\n",
       " 583: 'innermost',\n",
       " 584: 'foe',\n",
       " 585: 'throes',\n",
       " 586: 'valise',\n",
       " 587: 'creek',\n",
       " 588: 'tasks',\n",
       " 589: 'keaton',\n",
       " 590: 'para',\n",
       " 591: 'halleluiah',\n",
       " 592: 'neuva',\n",
       " 593: 'unbeliever',\n",
       " 594: 'clears',\n",
       " 595: 'reduction',\n",
       " 596: 'criminally',\n",
       " 597: 'aljanha',\n",
       " 598: 'anything',\n",
       " 599: \"getin'\",\n",
       " 600: 'knots',\n",
       " 601: \"wedon't\",\n",
       " 602: 'button',\n",
       " 603: 'owes',\n",
       " 604: 'assuring',\n",
       " 605: 'hueso',\n",
       " 606: \"khaki's\",\n",
       " 607: \"o'brians\",\n",
       " 608: 'methods',\n",
       " 609: \"bidin'\",\n",
       " 610: 'grits',\n",
       " 611: 'matters',\n",
       " 612: 'dotting',\n",
       " 613: 'dorit',\n",
       " 614: \"'i'\",\n",
       " 615: 'curves',\n",
       " 616: 'kickapoo',\n",
       " 617: 'lil',\n",
       " 618: \"give'em\",\n",
       " 619: 'tko',\n",
       " 620: 'crab',\n",
       " 621: 'ash',\n",
       " 622: 'professing',\n",
       " 623: 'downer',\n",
       " 624: 'alms',\n",
       " 625: 'wouldsing',\n",
       " 626: 'kapintasan',\n",
       " 627: 'anak',\n",
       " 628: 'hungred',\n",
       " 629: 'rampage',\n",
       " 630: 'athletic',\n",
       " 631: \"swear'\",\n",
       " 632: 'grime',\n",
       " 633: 'cursin',\n",
       " 634: 'meets',\n",
       " 635: 'trifles',\n",
       " 636: 'sprout',\n",
       " 637: 'bien',\n",
       " 638: 'foxes',\n",
       " 639: 'ymcmb',\n",
       " 640: 'spawned',\n",
       " 641: 'opponent',\n",
       " 642: 'seas',\n",
       " 643: 'flair',\n",
       " 644: 'disperses',\n",
       " 645: 'bansang',\n",
       " 646: 'proud',\n",
       " 647: 'cameras',\n",
       " 648: 'unmarked',\n",
       " 649: \"wreckin'\",\n",
       " 650: 'tortoise',\n",
       " 651: 'knocked',\n",
       " 652: 'diminishing',\n",
       " 653: 'lear',\n",
       " 654: 'weaponize',\n",
       " 655: 'impressing',\n",
       " 656: 'megan',\n",
       " 657: 'baile',\n",
       " 658: 'gott',\n",
       " 659: 'bloodedness',\n",
       " 660: 'spinnings',\n",
       " 661: 'andreas',\n",
       " 662: 'uncounted',\n",
       " 663: \"babyit's\",\n",
       " 664: 'flurry',\n",
       " 665: 'finale',\n",
       " 666: 'commencer',\n",
       " 667: 'parang',\n",
       " 668: 'hoard',\n",
       " 669: 'neglected',\n",
       " 670: 'natin',\n",
       " 671: 'hoochy',\n",
       " 672: 'uranus',\n",
       " 673: 'behold',\n",
       " 674: \"gigglin'\",\n",
       " 675: 'distantly',\n",
       " 676: 'excited',\n",
       " 677: 'meunder',\n",
       " 678: 'nakalaan',\n",
       " 679: 'dijon',\n",
       " 680: \"pyramid's\",\n",
       " 681: 'arow',\n",
       " 682: 'andyour',\n",
       " 683: 'roadrunner',\n",
       " 684: \"porsche's\",\n",
       " 685: 'equilibrius',\n",
       " 686: 'thegiant',\n",
       " 687: 'duchess',\n",
       " 688: 'weavers',\n",
       " 689: 'kilgore',\n",
       " 690: 'thrilled',\n",
       " 691: \"nodon't\",\n",
       " 692: 'kusesali',\n",
       " 693: 'braille',\n",
       " 694: 'idamean',\n",
       " 695: 'craved',\n",
       " 696: 'weaken',\n",
       " 697: 'tambah',\n",
       " 698: 'montsegur',\n",
       " 699: 'bozzio',\n",
       " 700: 'spawning',\n",
       " 701: \"'twist\",\n",
       " 702: 'strays',\n",
       " 703: \"whinin'\",\n",
       " 704: 'erases',\n",
       " 705: 'bally',\n",
       " 706: 'luckily',\n",
       " 707: 'tongs',\n",
       " 708: 'sleepy',\n",
       " 709: 'chesney',\n",
       " 710: 'sumpaan',\n",
       " 711: \"click's\",\n",
       " 712: 'vulgar',\n",
       " 713: 'steinway',\n",
       " 714: 'fifteen',\n",
       " 715: 'alala',\n",
       " 716: 'flu',\n",
       " 717: 'rundeeper',\n",
       " 718: 'dreary',\n",
       " 719: 'skinniest',\n",
       " 720: 'woohoo',\n",
       " 721: 'againi',\n",
       " 722: 'rhino',\n",
       " 723: 'puke',\n",
       " 724: 'swear',\n",
       " 725: 'chemo',\n",
       " 726: 'grills',\n",
       " 727: 'lahi',\n",
       " 728: 'mantle',\n",
       " 729: 'onversa',\n",
       " 730: 'oldy',\n",
       " 731: 'ballrooms',\n",
       " 732: 'hellhole',\n",
       " 733: 'mayall',\n",
       " 734: 'mulieribus',\n",
       " 735: 'palate',\n",
       " 736: 'talo',\n",
       " 737: 'rudely',\n",
       " 738: 'limpio',\n",
       " 739: 'simple',\n",
       " 740: 'launching',\n",
       " 741: 'nettie',\n",
       " 742: 'gin',\n",
       " 743: 'emancipation',\n",
       " 744: 'dressd',\n",
       " 745: 'fendi',\n",
       " 746: 'seemed',\n",
       " 747: 'bustling',\n",
       " 748: 'wondring',\n",
       " 749: 'trickling',\n",
       " 750: 'brag',\n",
       " 751: 'shield',\n",
       " 752: 'retained',\n",
       " 753: 'trickle',\n",
       " 754: 'fretting',\n",
       " 755: 'chaotic',\n",
       " 756: 'cavities',\n",
       " 757: 'squish',\n",
       " 758: 'abort',\n",
       " 759: 'egg',\n",
       " 760: 'catfish',\n",
       " 761: 'molecules',\n",
       " 762: 'rubs',\n",
       " 763: 'mureowa',\n",
       " 764: 'souviens',\n",
       " 765: \"others'\",\n",
       " 766: 'reincarnation',\n",
       " 767: 'sallow',\n",
       " 768: 'venue',\n",
       " 769: 'catches',\n",
       " 770: 'songsthat',\n",
       " 771: 'frontof',\n",
       " 772: 'output',\n",
       " 773: 'reeee',\n",
       " 774: 'bumping',\n",
       " 775: 'izabella',\n",
       " 776: 'seducee',\n",
       " 777: 'completely',\n",
       " 778: \"bobby's\",\n",
       " 779: 'limes',\n",
       " 780: 'throught',\n",
       " 781: 'asks',\n",
       " 782: 'murung',\n",
       " 783: 'dame',\n",
       " 784: 'maybel',\n",
       " 785: 'inmemphis',\n",
       " 786: 'carson',\n",
       " 787: \"i'a\",\n",
       " 788: 'goats',\n",
       " 789: 'storage',\n",
       " 790: 'room',\n",
       " 791: 'brought',\n",
       " 792: 'spoiled',\n",
       " 793: 'worshipping',\n",
       " 794: 'belonging',\n",
       " 795: \"'bove\",\n",
       " 796: 'corny',\n",
       " 797: 'comehere',\n",
       " 798: 'aminin',\n",
       " 799: 'combustication',\n",
       " 800: \"knew'd\",\n",
       " 801: 'unfriendly',\n",
       " 802: 'jeunes',\n",
       " 803: 'sault',\n",
       " 804: 'aparade',\n",
       " 805: 'monogrammed',\n",
       " 806: 'fixate',\n",
       " 807: 'hogger',\n",
       " 808: 'modeled',\n",
       " 809: \"cat's\",\n",
       " 810: 'library',\n",
       " 811: 'droning',\n",
       " 812: 'completly',\n",
       " 813: 'michael',\n",
       " 814: \"dunkin'\",\n",
       " 815: 'remotes',\n",
       " 816: 'inclination',\n",
       " 817: 'drifter',\n",
       " 818: 'verras',\n",
       " 819: 'bleibt',\n",
       " 820: 'dugo',\n",
       " 821: 'muscled',\n",
       " 822: 'il',\n",
       " 823: 'puccini',\n",
       " 824: 'eichmann',\n",
       " 825: 'grasses',\n",
       " 826: 'rigidly',\n",
       " 827: 'justified',\n",
       " 828: 'contain',\n",
       " 829: 'backdrop',\n",
       " 830: 'quelques',\n",
       " 831: 'wronged',\n",
       " 832: 'rivulets',\n",
       " 833: 'boughs',\n",
       " 834: 'deformed',\n",
       " 835: '50s',\n",
       " 836: 'linger',\n",
       " 837: 'jailbird',\n",
       " 838: 'ditty',\n",
       " 839: 'everett',\n",
       " 840: 'fuelled',\n",
       " 841: \"streakin'\",\n",
       " 842: 'beads',\n",
       " 843: 'perspective',\n",
       " 844: 'netoia',\n",
       " 845: 'omens',\n",
       " 846: 'port',\n",
       " 847: 'assist',\n",
       " 848: 'flowing',\n",
       " 849: \"jed's\",\n",
       " 850: 'basin',\n",
       " 851: 'cristina',\n",
       " 852: 'salinas',\n",
       " 853: \"minute's\",\n",
       " 854: 'falsely',\n",
       " 855: 'iser',\n",
       " 856: 'orphans',\n",
       " 857: \"tommie's\",\n",
       " 858: 'mapupulot',\n",
       " 859: '3p',\n",
       " 860: 'abel',\n",
       " 861: 'sundays',\n",
       " 862: 'brightened',\n",
       " 863: \"chick's\",\n",
       " 864: 'perignon',\n",
       " 865: 'vassals',\n",
       " 866: 'retreating',\n",
       " 867: 'decadence',\n",
       " 868: 'molten',\n",
       " 869: 'yourbig',\n",
       " 870: 'limelight',\n",
       " 871: 'ideas',\n",
       " 872: 'hap',\n",
       " 873: 'affidavit',\n",
       " 874: \"handbasket'cause\",\n",
       " 875: 'lyinest',\n",
       " 876: 'stankies',\n",
       " 877: 'hugged',\n",
       " 878: 'away',\n",
       " 879: 'colaiuta',\n",
       " 880: 'reggae',\n",
       " 881: 'clappin',\n",
       " 882: 'articulate',\n",
       " 883: 'annegher',\n",
       " 884: 'methadone',\n",
       " 885: 'katinuan',\n",
       " 886: 'daddle',\n",
       " 887: 'toured',\n",
       " 888: 'fanned',\n",
       " 889: 'reviews',\n",
       " 890: 'price',\n",
       " 891: \"sheriff's\",\n",
       " 892: 'ty',\n",
       " 893: 'commas',\n",
       " 894: 'partridge',\n",
       " 895: 'houdini',\n",
       " 896: 'gipeojin',\n",
       " 897: 'monument',\n",
       " 898: \"cheatin'\",\n",
       " 899: \"dalawa'y\",\n",
       " 900: 'loco',\n",
       " 901: 'naguguluhan',\n",
       " 902: 'obstruct',\n",
       " 903: 'beggars',\n",
       " 904: 'side',\n",
       " 905: 'glues',\n",
       " 906: \"nothing'\",\n",
       " 907: 'heats',\n",
       " 908: \"payday's\",\n",
       " 909: 'datang',\n",
       " 910: 'byebye',\n",
       " 911: 'tilled',\n",
       " 912: 'tarts',\n",
       " 913: 'veranda',\n",
       " 914: 'realize',\n",
       " 915: 'hudson',\n",
       " 916: \"apple'\",\n",
       " 917: 'prejudices',\n",
       " 918: 'cheerleader',\n",
       " 919: 'ignores',\n",
       " 920: 'undressing',\n",
       " 921: 'providing',\n",
       " 922: 'premiers',\n",
       " 923: 'donuts',\n",
       " 924: 'gardening',\n",
       " 925: 'pip',\n",
       " 926: 'savious',\n",
       " 927: 'raincoats',\n",
       " 928: 'tay',\n",
       " 929: 'buh',\n",
       " 930: 'itgo',\n",
       " 931: 'wiheomhan',\n",
       " 932: 'warmth',\n",
       " 933: 'religium',\n",
       " 934: 'tinggalkan',\n",
       " 935: 'tugu',\n",
       " 936: 'neither',\n",
       " 937: 'gu',\n",
       " 938: 'endures',\n",
       " 939: 'stubby',\n",
       " 940: 'defector',\n",
       " 941: 'sideshow',\n",
       " 942: 'blackness',\n",
       " 943: '18th',\n",
       " 944: 'nama',\n",
       " 945: 'slurpy',\n",
       " 946: 'guinevere',\n",
       " 947: \"homegirls'll\",\n",
       " 948: 'sirree',\n",
       " 949: 'crafting',\n",
       " 950: 'lynch',\n",
       " 951: 'slimm',\n",
       " 952: 'prudeness',\n",
       " 953: 'bossa',\n",
       " 954: 'seed',\n",
       " 955: 'assimilate',\n",
       " 956: 'questo',\n",
       " 957: 'accomplishments',\n",
       " 958: 'spoilt',\n",
       " 959: 'lethal',\n",
       " 960: 'burg',\n",
       " 961: 'abolish',\n",
       " 962: 'slashed',\n",
       " 963: 'joke',\n",
       " 964: 'hairy',\n",
       " 965: 'malins',\n",
       " 966: 'heavenly',\n",
       " 967: 'duller',\n",
       " 968: 'cypress',\n",
       " 969: 'devolve',\n",
       " 970: 'rammbled',\n",
       " 971: 'interstates',\n",
       " 972: 'exudes',\n",
       " 973: 'contender',\n",
       " 974: \"eternity'\",\n",
       " 975: 'testo',\n",
       " 976: \"dashboard's\",\n",
       " 977: 'typeface',\n",
       " 978: 'fettered',\n",
       " 979: 'himes',\n",
       " 980: 'mind',\n",
       " 981: 'thanks',\n",
       " 982: 'sensitivity',\n",
       " 983: 'julliard',\n",
       " 984: 'russ',\n",
       " 985: \"e'nuff\",\n",
       " 986: 'casting',\n",
       " 987: '13th',\n",
       " 988: 'fewer',\n",
       " 989: 'mundane',\n",
       " 990: 'vanderbilt',\n",
       " 991: 'swish',\n",
       " 992: \"mamma's\",\n",
       " 993: 'tensions',\n",
       " 994: 'fingeroos',\n",
       " 995: \"stuffin'\",\n",
       " 996: 'postman',\n",
       " 997: 'outlet',\n",
       " 998: 'shitten',\n",
       " 999: 'sanctities',\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id, id_to_word = mapping(tokenized_songs)\n",
    "print(\"Total Unique words: \", len(word_to_id))\n",
    "print(\"Mapping preview:\")\n",
    "word_to_id\n",
    "id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### **One hot Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(id, vocab_size):\n",
    "    base_vector = [0] * vocab_size\n",
    "    base_vector[id] = 1\n",
    "    return base_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Creating Training Dataset**\n",
    "\n",
    "For this we first need to create the X,Y pairs from the song lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(tokenized_songs, window):\n",
    "    pairs = []\n",
    "\n",
    "    for tokenized_song in tokenized_songs:\n",
    "        for tokenized_paragraph in tokenized_song:\n",
    "            for k, word in enumerate(tokenized_paragraph):\n",
    "                start = max(0, k - window)\n",
    "                end = min(len(tokenized_paragraph), k + window + 1)\n",
    "\n",
    "                for index in range(start, end):\n",
    "                    if index == k:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pairs.append((tokenized_paragraph[k], tokenized_paragraph[index]))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = generate_pairs(tokenized_songs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs Preview: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4075414"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Pairs Preview: \")\n",
    "pairs[:10]\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(batch, vocab_size, word_to_id):\n",
    "    X = []\n",
    "    y = []\n",
    "    for pair in batch:\n",
    "        X.append(one_hot_encode(word_to_id[pair[0]], vocab_size))\n",
    "        y.append(one_hot_encode(word_to_id[pair[1]], vocab_size))\n",
    "\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Creating Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3000\n",
    "mini_batches = []\n",
    "total_examples = len(pairs)\n",
    "num_of_batches = total_examples // batch_size\n",
    "leftover = total_examples % batch_size\n",
    "\n",
    "for i in range(num_of_batches):\n",
    "    start_index = i * batch_size\n",
    "    end_index = (i + 1) * batch_size\n",
    "    mini_batch = pairs[start_index:end_index]\n",
    "    mini_batches.append(mini_batch)\n",
    "\n",
    "if leftover > 0:\n",
    "    last_mini_batch = pairs[-leftover:]\n",
    "    mini_batches.append(last_mini_batch)\n",
    "\n",
    "# return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1359"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_batches[0])\n",
    "len(mini_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = encode_data(mini_batches[i], len(word_to_id), word_to_id)\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.empty((10, 2048, len(word_to_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    X, y = encode_data(mini_batches[i], len(word_to_id), word_to_id)\n",
    "    np.append(test, X)\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 36605)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = np.random.randn(len(word_to_id), 300) * (np.sqrt(2. / 300))\n",
    "W2 = np.random.randn(300, len(word_to_id)) * (np.sqrt(2. / len(word_to_id)))\n",
    "W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    t = np.exp(x)\n",
    "    t_sum = np.sum(t, axis=1, keepdims=True)\n",
    "    a = t / t_sum\n",
    "    return a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(z, y):\n",
    "    return - np.sum(np.log(z) * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 31522.700815724213\n",
      "Loss: 31523.342339752777\n",
      "Loss: 31522.913073406322\n",
      "Loss: 31523.798469898225\n",
      "Loss: 31523.297664745107\n",
      "Loss: 31521.17921222732\n",
      "Loss: 31523.506463022255\n",
      "Loss: 31516.165782916123\n",
      "Loss: 31521.501674154493\n",
      "Loss: 31510.477611631257\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(mini_batches):\n\u001b[0;32m----> 3\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mencode_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mword_to_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_to_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         Z1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, W1)\n\u001b[1;32m      5\u001b[0m         Z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Z1, W2)\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mencode_data\u001b[0;34m(batch, vocab_size, word_to_id)\u001b[0m\n\u001b[1;32m      5\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(one_hot_encode(word_to_id[pair[\u001b[38;5;241m0\u001b[39m]], vocab_size))\n\u001b[1;32m      6\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(one_hot_encode(word_to_id[pair[\u001b[38;5;241m1\u001b[39m]], vocab_size))\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39masarray(y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for index, batch in enumerate(mini_batches):\n",
    "        X, y = encode_data(batch, len(word_to_id), word_to_id)\n",
    "        Z1 = np.dot(X, W1)\n",
    "        Z2 = np.dot(Z1, W2)\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        loss = cross_entropy(A2, y)\n",
    "        print(\"Loss:\", loss)\n",
    "\n",
    "        dA2 = - y / A2\n",
    "        dA2.shape\n",
    "\n",
    "        dZ2 = dA2 * (A2 * (1 - A2))\n",
    "        dW2 = np.dot(Z1.T, dZ2)\n",
    "        dW1 = np.dot(dZ2.T, Z1)\n",
    "\n",
    "        W1 -= 0.001 * dW1\n",
    "        W2 -= 0.001 * dW2\n",
    "\n",
    "        # dA2 = A - y\n",
    "        # dW2 = np.dot(Z1.T, dA2)\n",
    "        # dA1 = np.dot(dA2, W2.T)\n",
    "        # dW1 = np.dot(X.T, dA1)\n",
    "\n",
    "        # W1 -= 0.01 * dW1\n",
    "        # W2 -= 0.01 * dW2\n",
    "\n",
    "        # if index == 20:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Code \n",
    "### Rough Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA2 = - y / A2\n",
    "dA2.shape\n",
    "\n",
    "dZ2 = dA2 * (A2 * (1 - A2))\n",
    "dW2 = np.dot(Z1.T, dZ2)\n",
    "dW1 = np.dot(dZ2.T, Z1)\n",
    "\n",
    "W1 -= 0.01 * dW1\n",
    "W2 -= 0.01 * dW2\n",
    "dW1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "##### Ignore the method below it was designed to be faster but it is a bit slower due to some extra computations. It functions the same as the above method just the approach is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tally = [0]\n",
    "\n",
    "songs = []\n",
    "\n",
    "tokens = []\n",
    "para_count = 0\n",
    "para_prev_length = 0\n",
    "\n",
    "for index, song in df.iterrows():\n",
    "    song_lyrics_paragraphs = song[\"text\"].replace(\"\\n\\n\", \"\").split(\"\\n  \\n\")\n",
    "\n",
    "    for paragraph in song_lyrics_paragraphs:\n",
    "        para_count += 1\n",
    "        tokenized_paragraph = tokenize(paragraph.replace(\"\\n\", \"\").replace(\"  \", \" \"))\n",
    "        tokens.extend(tokenized_paragraph)\n",
    "        para_prev_length += len(tokenized_paragraph)\n",
    "        tally.append(para_prev_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs_fast(tokens, window):\n",
    "    pairs = []\n",
    "\n",
    "    for i, value in enumerate(tally):\n",
    "        if i+1 == len(tally):\n",
    "            break\n",
    "\n",
    "        for k in range(tally[i], tally[i+1]):\n",
    "            localized_k = k - tally[i]\n",
    "            start = max(0, localized_k - window)\n",
    "            end = min(tally[i+1] - tally[i], localized_k + window + 1)\n",
    "\n",
    "            for index in range(start, end):\n",
    "                if index == localized_k:\n",
    "                    continue\n",
    "                else:\n",
    "                    pairs.append((tokens[localized_k + tally[i]], tokens[index + tally[i]]))\n",
    "    print(pairs[0])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Generating a summary of words (Not required this was just to learn the Counter Collections Class)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_summary(all_tokens):\n",
    "    words_count_dict = Counter(all_tokens)\n",
    "    print(\"Top 3 most common words are:\", words_count_dict.most_common(3))\n",
    "    print(\"Total unique words are:\", len(words_count_dict))\n",
    "    print(\"Total number of words:\", sum(words_count_dict.values()))\n",
    "    print(\"Total number of words:\", len(all_tokens))\n",
    "\n",
    "    return words_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count_dict = get_words_summary(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 5],\n",
    "    [3, 3, 3, 3],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.exp(a)\n",
    "c = np.sum(a, axis=1, keepdims=True)\n",
    "\n",
    "a / c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_old(X):\n",
    "    res = []\n",
    "    for x in X:\n",
    "        exp = np.exp(x)\n",
    "        res.append(exp / exp.sum())\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
